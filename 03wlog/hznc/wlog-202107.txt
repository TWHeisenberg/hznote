## tips

可以用来测试的服务器：

10.10.10.109



释放buffer/cache的内存：

```
释放缓存区内存的方法
sync# 将内存中数据同步到磁盘

 a）清理pagecache（页面缓存）

# echo 1 > /proc/sys/vm/drop_caches     或者 # sysctl -w vm.drop_caches=1

　 b）清理dentries（目录缓存）和inodes
# echo 2 > /proc/sys/vm/drop_caches     或者 # sysctl -w vm.drop_caches=2
　 c）清理pagecache、dentries和inodes

# echo 3 > /proc/sys/vm/drop_caches     或者 # sysctl -w vm.drop_caches=3

# 如何限制buffer/cache的占用？
```

注意：

夜盘数据比较特殊,  比如 2021-04-01 21:00:00 ~ 2021-04-02 00:00:00 的数据, 其实是 2021-03-31 21:00:00 ~ 2021-04-01 00:00:00 的数据.  即夜盘一整天的开盘时间是 昨天 21:00

maven 官方下载地址：

https://archive.apache.org/dist/maven/maven-3/

服务器地址：

```
10.10.10.104   administrator  Admin123
10.10.10.102 administrator  Admin711406
```



个人开发的软件源代码需要每月备份到网络硬盘(在资源管理器输入\\10.10.10.3进入),用户名:各人中文姓名,密码:89024521



这个SVN地址是你的私人地址，你可以提交任何临时代码。
：https://10.10.10.102/svn/private_source/wangtao
账户名和密码是名字全拼。

项目 SVN 地址:
https://10.10.10.102/svn/量化系统/nc_tree_parse/nc_algo

confluence 地址：http://10.10.0.47:8090/

​	ftp:list_delete_rowkey.shb

	   历史股指期权 和 商品期权 数据的下载地址 ：
	   域名：tik-ftp.citicsf.com 端口：8371
	   电信IP：58.33.80.163     端口：8371
	   联通IP：140.206.97.123 端口：8371
	   李佳桧/83g3Mh7m

​	历史数据：

​		50ETF期权数据
​		CFFEX股指期货
​		DCE大连商品
​		CZC3郑州商品
​		SHFE上海商品

钉钉邮箱：wangdao6551@dingtalk.com

能诚账号：u00479密码：1

jira账号：u00479/123456

jira   ip地址更换成：http://10.10.0.47:8080/

svn账号和密码：liuzhenjiang/liuzhenjiang

hbase:
	hadoop-daemon.sh start datanode
	hbase-daemon.sh start regionserver
	tail  -n -500 /usr/local/hbase/logs/hbase-hadoop-regionserver-slave4.log

sql server:

10.10.10.8

​		properties.setProperty("user", "R16");
​		properties.setProperty("password", "R16_0625");

现有集群规模：共有七台机器，老机房两台，新机房五台。

老机房两台：

​     主机名：keep-0        IP：10.10.10.110    nc_007用户登录密码：nc_007

主机名：keep-1        IP：10.10.10.111       nc_008用户登录密码：nc_008

 ![image-20210510085345685](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210510085345685.png)

新机房五台：

​     主机名：master      IP：10.10.0.254        root用户登录密码：1

主机名：slave1        IP：10.10.0.119        root用户登录密码：1

主机名：slave2        IP：10.10.0.54        root用户登录密码：1

主机名：slave3        IP：10.10.0.253        hadoop用户登录密码：1

主机名：slave4        IP：10.10.0.252        hadoop用户登录密码：1

 

集群配置：

​     三台zookeeper，七台HDFS（master，slave1，上运行namenode，七台DataNode）

​     Master，slave3作为HBASE的HMaster节点，除master节点之外，六台起HRegionserver服务，HMaster节点需要另起ThriftServer服务来提供读取数据服务。集群启动步骤见《集群各项服务启动命令.txt》文件

Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas

HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview



git key: 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

搭建ceph跟solr:

```
IP：10.10.0.202
用户名：wangtao/root
密码：123
```





## 每天做的

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123  
	进程：RunOnlyGuzhi，
    	  
10.10.10.102 administrator/Admin711406  
	进程：GetMarketDataAndStore
	     GuPiaoReduce（RunDataMerge?）
```



## 20210701

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			8.9 TB (13.44%)
			53.34 TB (80.57%)
	检查进程， 下面一个都不能少：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

jps | wc -l
jps
	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33

```

1.商品历史数据补录

​		商品历史数据补录

​				ag,cf,y,bu,pta,sp,rb,fg,ap,eb,ni,al,hc,lu,p,i,jm,j,zc,tc,c

​				检查15年的			--ok

​				解压并继续发送15年的

​				解压16年的

导入太慢了，换种方式，直接生成hfile:

​	Bulkload 

​	https://blog.csdn.net/xiaohu21/article/details/108310612

​		先生成csv		--ok

​		上传到hdfs

​				创建目录：

​						hdfs dfs -mkdir -p /csv/input/2015

​				上传：

​						hdfs dfs -put hznc_mkdata-tickdata.csv /csv/input/2015

​				查看：

​						hdfs dfs -ls /csv/input/2015

​		hbase shell 生成hfile文件

```shell
hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv \
-Dimporttsv.separator=, \
-Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from'  \
-Dimporttsv.bulk.output=/csv/output/2015 \
hznc_mkdata:tickdata \
/csv/input/2015
```

​		

没有权限：

```
Exception in thread "main" org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=EXECUTE, inode="/user/root/.staging/job_1625124755907_0002":hadoop:supergroup:drwx------
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:320)
```



需要启动web-proxy:

yarn-daemon.sh start proxyserver

查看mr任务：

yarn application -list

杀掉任务：

yarn application -kill application_1625127398446_0003



实时k线程序本地内存溢出了？

```java
// Something really bad happened. We are on the send thread that will now die.
LOG.error("Internal AsyncProcess #" + id + " error for "
    + tableName + " processing for " + server, t);
throw new RuntimeException(t);
```

是否可以调节线程的大小

32 位 windows增加内存？



20210701

今天工作：

1.商品历史数据补录

​		准备修改程序，换个方式导历史数据：

​					1.修改程序将各周期数据生成本地csv		--ok

​					2.收盘后调用hbase自带的importTsv工具将csv转换成hbase基础文件hfile		--正在测试中

​					3.增量导入hfile文件

2.商品历史数据补录  --ok

明天工作：

1.测试hbase BulkLoad导入数据方式

2.补录股指历史数据，由于合约全称化升级导致6.1-6.17数据缺失



#########

## 20210702

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			8.9 TB (13.44%)
			53.34 TB (80.57%)
	检查进程， 下面一个都不能少：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

jps | wc -l
jps
	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33

实时k线内存溢出了？
2021-07-02 13:51:32 ERROR [hconnection-0x87af79-shared--pool1-t2640] - Internal AsyncProcess #34257 error for hznc_mkdata:1sdata processing for keep-1,60020,1622529047848
java.lang.RuntimeException: java.lang.OutOfMemoryError: unable to create new native thread
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:220)

线程有多少？ 272 ，那么修改批次后呢？

jni创建的线程？

减小堆内存， --ok,让出了300M
修改批次大小，改成1000  --ok
再测试看看吧
```

1.商品历史数据补录

​		商品历史数据补录

​				ag,cf,y,bu,pta,sp,rb,fg,ap,eb,ni,al,hc,lu,p,i,jm,j,zc,tc,c

​				继续发送15年的

导入太慢了，换种方式，直接生成hfile:

​	Bulkload 

​	https://blog.csdn.net/xiaohu21/article/details/108310612

​		先生成csv		--ok

​		上传到hdfs

​				创建目录：

​						hdfs dfs -mkdir -p /csv/input/2015

​				上传：

​						hdfs dfs -put hznc_mkdata-tickdata.csv /csv/input/2015

​				查看：

​						hdfs dfs -ls /csv/input/2015

​		hbase shell 生成hfile文件		--ok

```shell
hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv \
-Dimporttsv.separator=, \
-Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from'  \
-Dimporttsv.bulk.output=/csv/output/2015 \
hznc_mkdata:tickdata \
/csv/input/2015
```

​		

增量导入表：

hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /csv/output/2015  hznc_mkdata:tickdata

se

没有权限：

```
Exception in thread "main" org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=EXECUTE, inode="/user/root/.staging/job_1625124755907_0002":hadoop:supergroup:drwx------
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:320)
        
vim /etc/profile
export HADOOP_USER_NAME=hadoop
source /etc/profile
```



需要启动web-proxy:

yarn-daemon.sh start proxyserver

查看mr任务：

yarn application -list

杀掉任务：

yarn application -kill application_1625127398446_0003



本地生成csv文件， 多进程，电脑资源利用好

​	15年		--正在运行

​	16年，17年，18年， 先解压完

hbase增量导入，测试一下



2.线程池



并发编程：

LinkedBlockingQueue

CopyOnWriteArrayList



#########

## 20210705

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			8.9 TB (13.44%)
			53.34 TB (80.57%)
	检查进程， 下面一个都不能少：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

jps | wc -l
jps
	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


#########

## 20210702

​```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			8.9 TB (13.44%)
			53.34 TB (80.57%)
	检查进程， 下面一个都不能少：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

jps | wc -l
jps
	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33

实时k线内存溢出了？
2021-07-02 13:51:32 ERROR [hconnection-0x87af79-shared--pool1-t2640] - Internal AsyncProcess #34257 error for hznc_mkdata:1sdata processing for keep-1,60020,1622529047848
java.lang.RuntimeException: java.lang.OutOfMemoryError: unable to create new native thread
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:220)

线程有多少？ 272 ，那么修改批次后呢？

jni创建的线程？

减小堆内存， --ok,让出了300M
修改批次大小，改成1000  --ok
再测试看看吧
​```

离线补录数据的程序总是内存溢出？在regionserver
发的太快，热点问题？

```

1.商品历史数据补录

​		本地可以稳定发送起来，不会OOM

​		改成不保存元素的队列试试，应该是parseLine这个方法出现了内存泄露		--ok

10.08日的数据没有？直接到1009了？

2.安装ceph,fileserver, 可以上传下载文件

​			ceph 部署安装

IP：10.10.0.202
用户名：wangtao/root
密码：123



ubuntu 安装ceph:

https://www.cnblogs.com/wangmo/p/11420197.html

https://zhuanlan.zhihu.com/p/67832892



卸载ceph,重新安装：

```
ceph-deploy purge streaming
ceph-deploy purgedata streaming
ceph-deploy forgetkeys
```

磁盘删除分区：

删除osd:

https://zhuanlan.zhihu.com/p/73478455



查看pool对象：

rados ls  --pool=ncdata

下载：

rados get test-object-1 test-object-1.txt --pool=ncdata



​			fileserver2编译过

​			fileserver2安装



20210705

今天工作：

1.物流项目，搭建ceph存储服务器

2.商品数据补录，修改程序生成本地csv文件

明天工作：

1.物流项目，开发和搭建fileserver提供上传下载功能

2.商品数据补录，生成20年数据的csv



#########

## 20210706

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			8.85 TB (13.37%)
			53.38 TB (80.63%)
	检查进程， 下面一个都不能少：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

jps | wc -l
jps
	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


jni创建的线程？

减小堆内存， --ok,让出了300M
修改批次大小，改成1000  --ok
再测试看看吧
​```

离线补录数据的程序总是内存溢出？在regionserver
发的太快，热点问题？

```

1.商品历史数据补录， 20年-21年

​		改成不保存元素的队列试试		--ok

​		应该是parseLine这个方法出现了内存泄露问题

​		10.08日的数据没有？直接到1009了？

​		解压20年的，

2.安装ceph,fileserver, 可以上传下载文件

3.搭建solr,可以检索

​			ceph 部署安装		--ok

​			搭建fileserver服务， 用spring boot + mvn，支持提取文本和发送solr		--ok

上传：

​			10.10.0.202:8081/uploadFile:

```
meta:
[
    {
        "key": "filename",
        "value": "RELEASE.TXT",
        "copy_to_text": true
    }
]

file:
FILE

confParameters:


pool
kafka: topic
solr collection？
```

下载：

10.10.0.202:8081/getFile

10.10.0.202:8081/getFile?url=ceph://ncdata/6cb47296-aef3-4655-9f6f-9c37e149f7e4&type=data



上传：



solr检索：

1.按照文件名查询

filename:bbbbbbbbb.txt



2.按照目录（权限）查询

catalog:005







IP：10.10.0.202
用户名：wangtao/root
密码：123

ubuntu 安装ceph:

https://www.cnblogs.com/wangmo/p/11420197.html

https://zhuanlan.zhihu.com/p/67832892



卸载ceph,重新安装：

```
ceph-deploy purge streaming
ceph-deploy purgedata streaming
ceph-deploy forgetkeys
```

磁盘删除分区：

删除osd:

https://zhuanlan.zhihu.com/p/73478455



查看pool对象：

rados ls  --pool=ncdata

下载：

rados get test-object-1 test-object-1.txt --pool=ncdata



安装solr:

```
cd /opt/solr/solr-7.7.3/bin
bash solr start -p 8983 -force # 启动solr
export SOLR_HOME=/opt/solr/solr-7.7.3/server/solr
bash solr start -p 8983 -force -d /opt/solr/solr-7.7.3/server/solr

# 创建core
solr create -c ncdata -p 8983 -force
# 创建collection
solr create -c ncdata -d /opt/solr/solr-7.7.3/ncdata -s 1 -rf 1 -n testcollection2conf 
```





下载：

10.10.0.202:8081/getFile?url=ceph://ncdata/a9aad095-88f2-47e7-9a67-557039e95f78

上传：



#########

## 20210707

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			8.85 TB (13.37%)
			53.38 TB (80.63%)
	检查进程， 下面一个都不能少：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

jps | wc -l
jps
	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


jni创建的线程？

减小堆内存， --ok,让出了300M
修改批次大小，改成1000  --ok
再测试看看吧
​```

离线补录数据的程序总是内存溢出？在regionserver
发的太快，热点问题？

```

1.商品历史数据补录， 20年-21年

​		应该是parseLine这个方法出现了内存泄露问题

​		10.08日的数据没有？直接到1009了？		--有的

​		文件直接写到hdfs中							--ok

​		跑mapreduce程序，生成hfile

```shell
# 删除目录 hadoop dfs -rm -r /history/futures/2020 
# 上传到hdfs

上传到hdfs
hdfs dfs -mkdir -p /history/futures/2015/hznc_mkdata-1sdata
hdfs dfs -put hznc_mkdata-1sdata /history/futures/2015/hznc_mkdata-1sdata/
hdfs dfs -ls /csv/input/2015

# hbase shell 生成hfile文件


hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv \
-Dimporttsv.separator=, -Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice' -Dimporttsv.bulk.output=/history/futures-out/2020/hznc_mkdata-1sdata/202001 \
hznc_mkdata:1sdata \
/history/futures/2020/hznc_mkdata-1sdata/202001


hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=, -Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from'  -Dimporttsv.bulk.output=/csv/output/2015 hznc_mkdata:tickdata /csv/input/2015


# 增量导入hbase:
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-output/2015/hznc_mkdata-1sdata/  hznc_mkdata:1sdata

```



20210707

今天工作：

1.同步6.25-7.6的股指主连数据到sqlServer,由于新装服务器，同步失败

2.商品数据补录程序修改：替换消息队列。解决内存泄露的问题

明天工作：

1.继续商品历史数据补录



#########

## 20210708

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			9.66 TB (14.6%)
			52.79 TB (79.74%)
	检查进程， 下面一个都不能少：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

jps | wc -l
jps
	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


jni创建的线程？

减小堆内存， --ok,让出了300M
修改批次大小，改成1000  --ok
再测试看看吧
​```

离线补录数据的程序总是内存溢出？在regionserver
发的太快，热点问题？

```

1.商品历史数据补录， 20年-21年

​			本地生成csv，3个进程运行		--ok

​					优化，直接发送hdfs

​			21年的也发起来

​			mapreduce生成hfile,再测试下，可以用21 年1月份的跑

​			主连数据先关闭		--ok

​			考虑使用hive导入的方式？

​						importTsv方式好像即使指定了output,超过一定的大小也会直接导入hbase?

​			只要下班前不死就行了



#########

## 20210709

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			9.92 TB (14.99%)
			52.41 TB (79.16%)
	检查进程， 下面一个都不能少：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

jps | wc -l
jps
	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


jni创建的线程？

减小堆内存， --ok,让出了300M
修改批次大小，改成1000  --ok
再测试看看吧
​```

离线补录数据的程序总是内存溢出？在regionserver
发的太快，热点问题？

```

1.商品历史数据补录， 20年-21年

​			本地生成csv，3个进程运行		--ok

​					优化，直接发送hdfs		--ok

​					主连数据先关闭		--ok

​					mapreduce导入hbase,测试对比下，可以先发送一个小的，或者用测试表做



```shell
# 发送之前
# Current count: 210000, row: zn2202_1624809600
# 210109 row(s) in 10.3680 seconds

# 发送之后：
Current count: 210000, row: zn2202_1624809600
210109 row(s) in 8.1530 seconds

hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.log.bad.lines=true -Dimporttsv.skip.bad.lines=false -Dimporttsv.separator=, -Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from' -Dimporttsv.bulk.output=/history/futures-out/2021/hznc_mkdata-tickdata/ \
hznc_mkdata:tickdata \
/history/futures/2021/hznc_mkdata-tickdata/
# 只能扫描一级目录
# 最后少了一个字段：cf_data:from


# 增量导入hbase:
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2020/hznc_mkdata-1sdata/  hznc_mkdata:1sdata


# class org.apache.hadoop.hbase.mapreduce.TsvImporterMapper
# tick:
HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from



hadoop dfs -rm -r /history/futures/2020/*/202001
```

Bad Lines=12904??





#########

## 20210710

1.股指的主力重新导入			--ok

​	6.30开始的		--ok
​	batch delete

​	批量导入

​			IC, IH, IF

2.20年数据继续导入

移动文件：

```
hadoop fs -mv /history/futures/2020/hznc_mkdata-1sdata/*/*  /history/futures/2020/hznc_mkdata-1sdata/

# 重新命名
hadoop fs -mv /history/futures/2020/hznc_mkdata-1sdata/202002/hznc_mkdata-1sdata_00  /history/futures/2020/hznc_mkdata-1sdata/hznc_mkdata-1sdata_202002_00

hadoop fs -mv /history/futures/2020/hznc_mkdata-1sdata/202002/hznc_mkdata-1sdata_00  /history/futures/2020/hznc_mkdata-1sdata/hznc_mkdata-1sdata_202002_00
```



1.hive sql			--ok

需求：

表名：user_visit_action

​	在hive上导入数据集，编写一下SQL：

​	1.查询top10热门品类  （综合排名 =  点击数*20%+下单数*30%+支付数*50%）
​	2.top10热门品类中top10活跃session


​	要SQL文件和运行截图。



sql怎么写？

日期	用户id	sessionId	页面Id	时间戳	搜索	点击	支付	下单	城市ID	

2019-07-17_95_26070e87-1ad7-49a3-8fb3-cc741facaddf_37_2019-07-17 00:00:02_手机_-1_-1_null_null_null_null_3

先统计出每个品种的点击数，下单数和支付数

那就分开expload呗

```shell
# 创建表
CREATE TABLE user_visit_action(
     create_date STRING, 
     user_id BIGINT,
     session_id STRING, 
     page_id BIGINT,
     action_time STRING,
     search_keyword STRING,
     click_category_id STRING,
     click_product_id STRING,
     order_category_ids ARRAY<STRING>,
     order_product_ids ARRAY<STRING>,
     pay_category_ids ARRAY<STRING>,
     pay_product_ids ARRAY<STRING>,
     city_id BIGINT)
 COMMENT 'this is user visit action table'
 ROW FORMAT DELIMITED
   FIELDS TERMINATED BY '_'
   COLLECTION ITEMS TERMINATED BY ',';
   
   
# 创建统计品种及其action的表
create TABLE category_statistics_info(
	category_id ,
	click_count BIGINT,
	pay_count BIGINT,
	order_count BIGINT
);
   
 # 加载数据
 load data local inpath '/opt/tmp/user_visit_action.txt' into table user_visit_action;
```

1.先安装hive 		--ok

2.加载数据到hive		--ok

3.查询		--ok

categoryId, orderCount, payCount



```sql
# 准备，创建源数据表和中间统计表
CREATE TABLE user_visit_action(
     create_date STRING, 
     user_id BIGINT,
     session_id STRING, 
     page_id BIGINT,
     action_time STRING,
     search_keyword STRING,
     click_category_id STRING,
     click_product_id STRING,
     order_category_ids ARRAY<STRING>,
     order_product_ids ARRAY<STRING>,
     pay_category_ids ARRAY<STRING>,
     pay_product_ids ARRAY<STRING>,
     city_id BIGINT)
 COMMENT 'this is user visit action table'
 ROW FORMAT DELIMITED
   FIELDS TERMINATED BY '_'
   COLLECTION ITEMS TERMINATED BY ',';
   
   
# 创建统计品种信息
create TABLE category_statistics_info(
	category_id STRING,
	click_count BIGINT,
	pay_count BIGINT,
	order_count BIGINT
);
   
 # 加载数据
 load data local inpath '/opt/tmp/user_visit_action.txt' into table user_visit_action;


# 1.统计每个品种的点击数，支付数和下单数加载到统计表
#   主要思想是先统计各个品种的点击数，支付数和订单数加载到统计表
#   1.1 分别统计出每个品种的点击数，支付数和订单数
#   1.2 因为每个品种有不同的行为，不同行为的点击数，支付数和订单数累加，得到一个品种完整的点击数，支付数和订单数
#   1.3 加载到统计表
insert into category_statistics_info 
	(SELECT tmp_result.category_id,
		sum(tmp_result.click_count),
		sum(tmp_result.pay_count),
		 sum(tmp_result.order_count) from
		(SELECT category_id,
		0 AS click_count,
		 0 AS pay_count,
		 count(1) AS order_count
		FROM 
			(SELECT *
			FROM user_visit_action
			WHERE size(order_category_ids) > 0
					AND order_category_ids[0] != "null" ) AS t LATERAL VIEW explode(t.order_category_ids) v AS category_id
			GROUP BY  category_id
			UNION
			allSELECT category_id,
		 0 AS click_count,
		 count(1) AS pay_count,
		 0 AS order_count
			FROM 
				(SELECT *
				FROM user_visit_action
				WHERE size(pay_category_ids) > 0
						AND pay_category_ids[0] != "null" ) AS t LATERAL VIEW explode(t.pay_category_ids) v AS category_id
				GROUP BY  category_id
				UNION
				allSELECT cast(click_category_id AS STRING) AS category_id,
		count(1) AS click_count,
		 0 AS pay_count,
		 0 AS order_count
				FROM user_visit_action
				WHERE click_category_id != -1
				GROUP BY  click_category_id )as tmp_result
				GROUP BY  tmp_result.category_id; ); 

# 2.按照规则查询top10的热门品种 （综合排名 =  点击数*20%+下单数*30%+支付数*50%）
#   2.1 按照热门的规则计算出score
#   2.2 按照score排序，top 10
select category_id, (click_count*0.2 + pay_count*0.5 + order_count*0.3) as score from category_statistics_info order by score desc limit 10;

# 3.查询top10热门品类中top10活跃session
# 	3.1 基于上一个步骤查询出来的top10品种
#   3.2 筛选出在top10品种对应的所有session
#   3.3 按照session分组，统计活跃次数并且按照次数降序排序,查出top10
SELECT u.session_id,
		 count(1) AS session_count
FROM user_visit_action AS u, 
	(SELECT category_id,
		 (click_count*0.2 + pay_count*0.5 + order_count*0.3) AS score
	FROM category_statistics_info
	ORDER BY  score DESC limit 10) AS r
WHERE u.click_category_id = r.category_id
		OR array_contains(u.pay_category_ids, r.category_id)
		OR array_contains(u.order_category_ids, r.category_id)
GROUP BY  session_id
ORDER BY  session_count DESC limit 10;

# 禁用类型的限制和严格模式：
set hive.strict.checks.cartesian.product=flase;

set hive.mapred.mode=nonstrict;

# 报错？
FAILED: SemanticException Cartesian products are disabled for safety reasons. If you know what you are doing, please sethive.strict.checks.cartesian.product to false and that hive.mapred.mode is not set to 'strict' to proceed. Note that if you may get errors or incorrect results if you make a mistake while using some of the unsafe features.

# 设置		--ok
#将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询,为了保证集群的稳定
set hive.strict.checks.cartesian.product=false; 

#  order by 会额外增加一次mapreduce任务
```



3.21年的继续发：

​			下载解压

​			运行起来



## 20210712

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			9.66 TB (14.6%)
			52.79 TB (79.74%)
	检查进程， 下面一个都不能少：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

jps | wc -l
jps
	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


jni创建的线程？

减小堆内存， --ok,让出了300M
修改批次大小，改成1000  --ok
再测试看看吧
​```

离线补录数据的程序总是内存溢出？在regionserver
发的太快，热点问题？

删除除了股指的主力代码：
delete from dbo.futures_main_contract where '2021-07-12' >= start_date AND '2021-07-12' <= end_date AND future_variety NOT IN ('IC','IH','IF');

```

本周工作：

1.商品历史数据继续补录：21年的补完，继续补录19年之前的

2.实时k线程序开发，运行两套程序，一套写本地，一套发hbase。



1.商品历史数据补录：

​			20年的补完

hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2020/hznc_mkdata-2sdata/  hznc_mkdata:2sdata

​			21年的补完

```

2021/07/12 09:47:14 INFO  [Thread-74] - Exception in createBlockOutputStream
java.io.EOFException: Premature EOF: no length prefix available
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2282)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1343)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1184)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:454)
2021/07/12 09:47:14 WARN  [Thread-74] - Error Recovery for block BP-820644543-10.10.0.254-1562028117422:blk_1076778191_3439213 in pipeline DatanodeInfoWithStorage[10.10.10.110:50010,DS-b5adc1a0-fa73-4510-a1e3-df5958cbdef0,DISK], DatanodeInfoWithStorage[10.10.10.7:50010,DS-eb506daf-e97f-4817-a47a-1bfad1ccab24,DISK], DatanodeInfoWithStorage[10.10.0.254:50010,DS-01819ff3-47d7-42ee-8f27-929b56caade1,DISK]: bad datanode DatanodeInfoWithStorage[10.10.10.110:50010,DS-b5adc1a0-fa73-4510-a1e3-df5958cbdef0,DISK]


2021/07/12 09:53:43 INFO  [Thread-588] - Exception in createBlockOutputStream
java.io.IOException: Got error, status message , ack with firstBadLink as 10.10.10.110:50010
        at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:142)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1359)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1184)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:454)
2021/07/12 09:53:43 WARN  [Thread-588] - Error Recovery for block BP-820644543-10.10.0.254-1562028117422:blk_1076778469_3440925 in pipeline DatanodeInfoWithStorage[10.10.0.119:50010,DS-48e2d2a3-17e7-45b7-a488-0f9677b20ca5,DISK], DatanodeInfoWithStorage[10.10.10.110:50010,DS-b5adc1a0-fa73-4510-a1e3-df5958cbdef0,DISK], DatanodeInfoWithStorage[10.10.10.7:50010,DS-27914513-a01a-43b1-b5cd-d8b8941c60e2,DISK]: bad datanode DatanodeInfoWithStorage[10.10.10.110:50010,DS-b5adc1a0-fa73-4510-a1e3-df5958cbdef0,DISK]
磁盘有问题？

Left over 7 task(s) are processed on server(s): [hadoop01,60020,1625564766092, slave1,60020,1626055436842]

可能是region server挂掉了进行region move操作引起的？ 
	怎么优化，避免这个问题？
```

​			19年的继续补录

​			bug处理，发生了死锁？





## 20210713

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			9.66 TB (14.6%)
			52.79 TB (79.74%)
	检查进程， 下面一个都不能少：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

jps | wc -l
jps
	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


jni创建的线程？

减小堆内存， --ok,让出了300M
修改批次大小，改成1000  --ok
再测试看看吧
​```

离线补录数据的程序总是内存溢出？在regionserver
发的太快，热点问题？


cpu很高：
root@master:~# ps -ef | grep xmrig
root     24304 22635  0 09:11 pts/11   00:00:00 grep --color=auto xmrig
root     24905     1 99 7月09 ?       12-04:32:48 /tmp/q/./xmrig -o 139.99.124.170:80 -u 4BrL51JCc9NGQ71kWhnYoDRffsDZy7m1HUU7MRU4nUMXAHNFBEJhkTZV9HdaL4gfuNBxLPc3BeMkLGaPbF5vWtANQumMny5XCn4Ndx9LDi --donate-level 0 -p 1
```

2.商品数据继续补录

​	20年的补完		--ok

```
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2020/hznc_mkdata-15sdata/  hznc_mkdata:15sdata
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2020/hznc_mkdata-30sdata/  hznc_mkdata:30sdata
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2020/hznc_mkdata-1mindata/  hznc_mkdata:1mindata
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2020/hznc_mkdata-5mindata/  hznc_mkdata:5mindata
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2020/hznc_mkdata-15mindata/  hznc_mkdata:15mindata
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2020/hznc_mkdata-30mindata/  hznc_mkdata:30mindata
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2020/hznc_mkdata-60mindata/  hznc_mkdata:60mindata
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2020/hznc_mkdata-ddata/  hznc_mkdata:ddata
```

​	21年的补完， 发送完了，生成hfile

```

2021-07-13 13:30:58,239 INFO  [main] mapreduce.Job: Task Id : attempt_1625643627302_0138_r_000011_1, Status : FAILED
Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#24
        at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:56
        
        shuffle阶段reduce端从map端拉取数据，先放到内存中，内存不够了，可以用参数mapreduce.reduce.shuffle.memory.limit.percent控制，达到一定内存就使用磁盘，默认是0.25，改成0.1：
        mapreduce.reduce.shuffle.memory.limit.percent=0.1
       
       
```

​	19年的发送起来



2.查一下昨天IC2107, 10:10 5min k线价格不对的问题， IC2107_1626055800				--ok

​		对比下生成时间：2021-07-12 10:29:58 相差了20min？

​		看看其他时间：10:05 -- 2021-07-12 10:10:01		--正确的

​									10:15 -- 	2021-07-12 11:25:26		--这个是后来补的， 价格是正确的

​		看看同周期的1分钟k线

​									10:10  -- 2021-07-12 10:11:02  --价格也是对的

单元测试			--单元测试也是对的？

开盘价和最低价不对， 自动补k的		--ok

盘中补录数据要考虑各个周期的数据。

4.实时程序改进，一套保存到本地。

​			10：10 - 10：15



5.实时程序改进，增加自旋时间解决延迟问题		--明天测试一下

​		最大自旋时间？一个周期的时间

​				1s: 1s， 2s:2s, 3s:3s, 1min:1min

6.处理股票数据，1min合成60min数据



20210712

今天工作：

1.历史商品21年5-6月的数据运行生成csv文件		--预计花费一天时间

2.历史商品19年的数据下载解压ok，启动运行生成csv文件		--预计花费3天时间

明天工作：

  1.历史商品数据继续补录 

2. 实时程序改进，支持数据生成到本地csv文件



## 20210714

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			50.17 TB (75.79%)
	检查进程， 下面一个都不能少：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

jps | wc -l
jps
	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


jni创建的线程？

减小堆内存， --ok,让出了300M
修改批次大小，改成1000  --ok
再测试看看吧
​```

离线补录数据的程序总是内存溢出？在regionserver
发的太快，热点问题？


cpu很高：
root@master:~# ps -ef | grep xmrig
root     24304 22635  0 09:11 pts/11   00:00:00 grep --color=auto xmrig
root     24905     1 99 7月09 ?       12-04:32:48 /tmp/q/./xmrig -o 139.99.124.170:80 -u 4BrL51JCc9NGQ71kWhnYoDRffsDZy7m1HUU7MRU4nUMXAHNFBEJhkTZV9HdaL4gfuNBxLPc3BeMkLGaPbF5vWtANQumMny5XCn4Ndx9LDi --donate-level 0 -p 1
```

2.商品数据继续补录

​		21年的补录完		--ok

```
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2021/hznc_mkdata-10sdata  hznc_mkdata:10sdata 
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2021/hznc_mkdata-15sdata  hznc_mkdata:15sdata 
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2021/hznc_mkdata-30sdata  hznc_mkdata:30sdata 
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2021/hznc_mkdata-1mindata  hznc_mkdata:1mindata 
```

​		19年的继续生产csv



3.股票数据处理，生产60min k线				--ok

```
服务器ip:10.10.10.8
用户名:read2
密码: read253196

DATA_NoRest.dbo.MIN01_2014-2021 为除权一分钟数据

wenbu

CREATE TABLE [dbo].[min01_tmp](
	[代码] [char](6) NOT NULL,
	[日期] [smalldatetime]  NOT NULL,
	[开盘价] [numeric](9, 2)  NOT NULL,
	[最低价] [numeric](9, 2) NOT NULL,
	[收盘价] [numeric](9, 2) NOT NULL,
	[最高价] [numeric](9, 2) NOT NULL,
	[成交量] [bigint]  NOT NULL,
	[成交金额] [bigint]  NOT NULL,
)


```

4.测试实时k线程序，缓存延迟问题			--ok

​		并发量

​		是否有延迟？查看hbase入库时间



5.临时的数据补录		--ok



## 20210715

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			12.39 TB (18.71%)
			49.9 TB (75.37%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


jni创建的线程？

减小堆内存， --ok,让出了300M
修改批次大小，改成1000  --ok
再测试看看吧
​```

离线补录数据的程序总是内存溢出？在regionserver
发的太快，热点问题？


主力切换成2108了？对比下看看
```

2.商品数据继续补录

​		19年的生成csv		--ok

​		19年的开始生成hfile文件

```shell
hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.log.bad.lines=true -Dimporttsv.skip.bad.lines=false -Dimporttsv.separator=, -Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from' -Dimporttsv.bulk.output=/history/futures-out/2015/hznc_mkdata-tickdata/ \
hznc_mkdata:tickdata \
/history/futures/2015/hznc_mkdata-tickdata/
# 只能扫描一级目录
# 最后少了一个字段：cf_data:from


# 增量导入hbase:
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2019/hznc_mkdata-tickdata  hznc_mkdata:tickdata


# class org.apache.hadoop.hbase.mapreduce.TsvImporterMapper
# tick:
HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from


hadoop dfs -rm -r /history/futures/2020/*/202001
```



3.升级实时k线程序，解决缓存延迟的问题		--ok



## 20210716

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			12.57 TB (18.98%)
			49.71 TB (75.09%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


jni创建的线程？

减小堆内存， --ok,让出了300M
修改批次大小，改成1000  --ok
再测试看看吧
​```

离线补录数据的程序总是内存溢出？在regionserver
发的太快，热点问题？


主力切换成2108了？对比下看看
```

2.商品数据继续补录

​		19年的继续生产hfile			--ok

```shell
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2019/hznc_mkdata-3sdata  hznc_mkdata:3sdata		--ok
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2019/hznc_mkdata-5sdata  hznc_mkdata:5sdata		--ok

hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2019/hznc_mkdata-10sdata  hznc_mkdata:10sdata	--ok
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2019/hznc_mkdata-15sdata  hznc_mkdata:15sdata  --ok
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2019/hznc_mkdata-30sdata  hznc_mkdata:30sdata  --ok
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2019/hznc_mkdata-1mindata  hznc_mkdata:1mindata  --ok
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2019/hznc_mkdata-5mindata  hznc_mkdata:5mindata  --ok
```

​		18年的开始运行

​		优先吧香蕉补完，考虑先筛选出来

​		15年的导进去

```shell
hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.log.bad.lines=true -Dimporttsv.skip.bad.lines=false -Dimporttsv.separator=, -Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from' -Dimporttsv.bulk.output=/history/futures-out/2015/hznc_mkdata-tickdata/ \
hznc_mkdata:tickdata \
/history/futures/2015/hznc_mkdata-tickdata/
# 只能扫描一级目录
# 最后少了一个字段：cf_data:from


# 增量导入hbase:
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2019/hznc_mkdata-tickdata  hznc_mkdata:tickdata


```

3.实时k线程序观察

​		写到本地的数据		--ok

​		是否还有延迟



4.资金流向的需求整理

问题：只有20年11月份开始的



表：

hznc_tgdata:amount

hznc_tgdata:bigorder

代码：股票代码

时间戳：最后的时间，例如将9.30-10.30 时间段的数据合成后取10.30

委买金额：9.30-10.30时间段（tick级别） 所有委买金额 累加

委卖金额：9.30-10.30时间段 委卖金额 累加

主动买金额：9.30-10.30时间段 主动买金额 累加

主动卖金额：9.30-10.30时间段 主动卖金额 累加

委买小单: 9.30-10.30时间段内的tick级别数据 委买金额小于5w的累加

委买中单：  5w-20w

委买大单   20w-100w

委买超大单  100w+

委卖大单：9.30-10.30时间段内的tick级别数据 委卖金额小于5w的累加

委卖中单  5w-20w

委卖大单：  20w-100w

委买超大单：  100w+



20210715

今天工作

1. 历史商品数据继续补录

   ​	19年的csv开始生成hfile, tick周期已经ok, 正常生成1s周期的hfile

   

   2.实时程序升级，解决缓存延迟的问题		--ok

   ​	增加一套程序，数据写到本地

   

   

   明天工作：

   ​	1.继续历史商品数据补录

   ​			19年的数据跑mapreduce任务生成hfile

   ​	
   
   



############

本周工作：

1.历史商品数据补录

​	1.21年的数据补录完，到6月31号的		--ok

​	2.20年数据补录完		--ok

​	3.继续补录19年的数据		--ok

2.实时程序改进，一套保存到本地，支持盘中补录数据		--ok



3.股票数据，1分钟合并成60分钟数据。

```
服务器ip:10.10.10.8
用户名:read2
密码: read253196

DATA_NoRest.dbo.MIN01_2014-2021 为除权一分钟数据
```





## 20210719

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			13.64 TB (20.61%)
			48.59 TB (73.4%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


```

2.补录历年的橡胶数据

​		读取zip文件，抽出来单独数据，生成原来目录结构的文件		--ok

​		生成csv文件

​		生成hfile文件并增量导入

​		然后把pta的补上

​	  15年的		--ok



3.本周计划			--ok

​	

4.继续zk和redis

​	zk 		--ok



20210720

今天工作：

​	补录商品橡胶数据， 生成csv文件写入hdfs 		--ok

​	补录18年商品数据，运行生成csv文件

​	修复hbase服务器down掉的问题

明天工作：

​	继续补录商品橡胶数据，生成hfile

​	18年商品数据继续补录

​	



​					

## 20210720

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			66.2 TB
			13.61 TB (20.56%)
			48.69 TB (73.55%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


```

2.补录商品数据

​		橡胶数据生成csv文件

​		18年商品数据，跑mr任务

```shell
hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.log.bad.lines=true -Dimporttsv.skip.bad.lines=false -Dimporttsv.separator=, -Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from' -Dimporttsv.bulk.output=/history/futures-out/2018/hznc_mkdata-tickdata/ \
hznc_mkdata:tickdata \
/history/futures/2018/hznc_mkdata-tickdata/
# 只能扫描一级目录
# 最后少了一个字段：cf_data:from


# 增量导入hbase:
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2018/hznc_mkdata-1sdata  hznc_mkdata:1sdata

# 远程调试看看：

```



3.

hbase

```shell
hbase hbck -repair table

hbase hbck # 检查hbase
    ERROR: There is a hole in the region chain between 000800_1893427200_2 and 000930_1893427200_2.  You need to create a new .regioninfo and region dir in hdfs to plug the hole.
    73 inconsistencies detected.
Status: INCONSISTENT

hbase hbck -fixAssignments
hbase hbck -fixAssignments # 把meta表中记录的region分配给regionserver 
    
    
hbase hbck 只做检查 
hbase hbck -fixMeta 根据region目录中的.regioninfo，生成meta表
hbase hbck -fixAssignments 把meta表中记录的region分配给regionserver 
hbase hbck -fixHdfsOrphans 修复.regioninfo文件
hbase hbck -repair  表名

```



## 20210721

```
1.检查下hadoop集群, hbase集群		--ok
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			67.09 TB
			14.18 TB (21.14%)
			48.4 TB (72.14%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


```

2.补录商品数据

​	橡胶数据，发完csv,生成hfile

​	18年数据继续生成hfile,等橡胶完成

```shell
hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.log.bad.lines=true -Dimporttsv.skip.bad.lines=false -Dimporttsv.separator=, -Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from' -Dimporttsv.bulk.output=/history/futures-out/2018/hznc_mkdata-tickdata/ \
hznc_mkdata:tickdata \
/history/futures/2018/hznc_mkdata-tickdata

hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.log.bad.lines=true -Dimporttsv.skip.bad.lines=false -Dimporttsv.separator=, -Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:from' -Dimporttsv.bulk.output=/history/futures-out/2018/hznc_mkdata-2sdata/ \
hznc_mkdata:2sdata \
/history/futures/2018/hznc_mkdata-2sdata
# 只能扫描一级目录
# 最后少了一个字段：cf_data:from

# 增量导入hbase:
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/ru/hznc_mkdata-1sdata  hznc_mkdata:1sdata
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/ru/hznc_mkdata-2sdata  hznc_mkdata:2sdata
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/ru/hznc_mkdata-5sdata  hznc_mkdata:5sdata

3s的表报错：
Can't read partitions file
Caused by: java.io.IOException: Wrong number of partitions in keyset
http://support-it.huawei.com/docs/zh-cn/fusioninsight-all/maintenance-guide/zh-cn_topic_0222551198.html


18年的1s		--ok
18年的tick    --nok, bad lines?

```

3.其他

​		

4.修复表异常：

```
hbase hbck hznc_mkdata:3sdata		--ok
hbase hbck -fixHdfsOverlaps
hbase hbck -fixHdfsOverlaps hznc_mkdata:3sdata

好东西：
FusionInsight HD 维护宝典.pdf


```





20210722

今天工作：

​	15-21年商品橡胶数据补录		--ok

​	补录18年商品数据

​			1s和2s  --ok

明天工作：

​	橡胶数据生成主力代码然后生成主连数据

​	继续补录18年商品其他周期数据

​	

## 20210722

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			67.09 TB
			12.7 TB (18.93%)
			49.89 TB (74.35%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


```

2.补录商品数据

​	橡胶数据，发完csv,生成hfile

​	18年数据继续生成hfile,等橡胶完成

```
2021-07-22 08:36:50,668 INFO  [main-EventThread] zookeeper.ClientCnxn: EventThread shut down				--ok
Exception in thread "main" java.io.IOException: Trying to load more than 32 hfiles to one family of one region
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.doBulkLoad(LoadIncrementalHFiles.java:430)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.doBulkLoad(LoadIncrementalHFiles.java:344)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.run(LoadIncrementalHFiles.java:1124)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.main(LoadIncrementalHFiles.java:1131)
        
       
       hbase-site.xlm
    <property> 
    <name>hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily</name> 
    <value>3200</value>
    </property>
    
    ru:
    tick, 1s, 2s, 3s, 5s ,10s,15s,30s， 1min，5min		--ok
		15min，30min,60min,ddata
		
    2018:
    1s	--ok
    
    
   [["1","0","1","0","0"],["1","0","1","1","1"],["1","1","1","1","1"],["1","0","0","1","0"]]
   

```

生成主力代码



##############

本周工作

1.补录历年的橡胶数据

2.继续补录18年的数据

3.2015年的补录完		--ok

​		3s的补录失败？



高稳定性：

保证程序的稳定性和正确性，个人提倡从测试出发保证，这个测试不是说开发依赖测试人员，

是开发首先自己要通过测试。不能说自己都没有测试过就交给测试人员，毕竟开发是主要责任人。

测试通过三个方面保证：

单元测试，很重要！

功能性的测试即集成测试

性能测试

目前环境性能测试没有充足的条件。其他自己都是可以去实现的。



## 20210723

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			67.09 TB
			12.97 TB (19.34%)
			49.56 TB (73.87%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


```

2.补录商品数据

​	橡胶数据生成主力代码		--ok

​		开始生成主连数据		--ok

```
使用方法:1.进入Index_Data_his数据库, 将需计算的数据存入“T003_主力合约待计算表”;
2、执行“C001_批量计算期指主力合约”存储过程;
3、查看“T004_主力合约代码”表
```



​	18年数据继续生成hfile

​		tick,1s,2s,3s,5s,10s，15s,30s			--ok

3.redis



今天工作-20210723：

计算生成橡胶数据主力代码		--ok

生成橡胶主连数据

下周工作：

​	生成橡胶主连数据

​	17年商品数据补录



## 20210724

```
1.检查下hadoop集群, hbase集群
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			67.09 TB
			12.97 TB (19.34%)
			49.56 TB (73.87%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


```

2.补录橡胶的主连数据

```shell
hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.log.bad.lines=true -Dimporttsv.skip.bad.lines=false -Dimporttsv.separator=, -Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from' -Dimporttsv.bulk.output=/history/futures-out/ru-main/hznc_mkdata-tickdata/ \
hznc_mkdata:tickdata \
/history/futures/ru-main/hznc_mkdata-tickdata

/history/futures/ru-main
```

3.补录股票数据			--ok

​		看看程序？

​				hznc_data:60mindata_1，hznc_data:Ddata_1

​				 hznc.com.java.NewInterfaceToHbase.GuPiaoReduce

​				沪深300 000300，中证500 000905 ，上证50  000016

​				补录日k和小时k



​		数据？

## 20210726

​	明天继续。。。



```java
//标准的如何在双向链表中将指定元素放入队尾
// LinkedHashMap 中覆写
//访问元素之后的回调方法

/**
 * 1. 使用 get 方法会访问到节点, 从而触发调用这个方法
 * 2. 使用 put 方法插入节点, 如果 key 存在, 也算要访问节点, 从而触发该方法
 * 3. 只有 accessOrder 是 true 才会调用该方法
 * 4. 这个方法会把访问到的最后节点重新插入到双向链表结尾
 */
void afterNodeAccess(Node<K,V> e) { // move node to last
    // 用 last 表示插入 e 前的尾节点
    // 插入 e 后 e 是尾节点, 所以也是表示 e 的前一个节点
    LinkedHashMap.Entry<K,V> last;
    //如果是访问序，且当前节点并不是尾节点
    //将该节点置为双向链表的尾部
    if (accessOrder && (last = tail) != e) {
        // p: 当前节点
        // b: 前一个节点
        // a: 后一个节点
        // 结构为: b <=> p <=> a
        LinkedHashMap.Entry<K,V> p =
            (LinkedHashMap.Entry<K,V>)e, b = p.before, a = p.after;
        // 结构变成: b <=> p <- a
        p.after = null;

        // 如果当前节点 p 本身是头节点, 那么头结点要改成 a
        if (b == null)
            head = a;
        // 如果 p 不是头尾节点, 把前后节点连接, 变成: b -> a
        else
            b.after = a;

        // a 非空, 和 b 连接, 变成: b <- a
        if (a != null)
            a.before = b;
        // 如果 a 为空, 说明 p 是尾节点, b 就是它的前一个节点, 符合 last 的定义
      	// 这个 else 没有意义，因为最开头if已经确保了p不是尾结点了，自然after不会是null
        else
            last = b;

        // 如果这是空链表, p 改成头结点
        if (last == null)
            head = p;
        // 否则把 p 插入到链表尾部
        else {
            p.before = last;
            last.after = p;
        }
        tail = p;
        ++modCount;
    }
}


				测试用例:["LRUCache","put","put","get","put","get","put","get","get","get"]
				[[2],[1,1],[2,2],[1],[3,3],[2],[4,4],[1],[3],[4]]
				测试结果:[null,null,null,1,null,2,null,-1,3,4]
				期望结果:[null,null,null,1,null,-1,null,-1,3,4]



广度优先搜索 / 深度优先搜索，二分查找，滑动窗口，双指针，动态规划
```



## 20210727

```
1.检查下hadoop集群, hbase集群		--ok
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			67.09 TB
			12.51 TB (19.29%)
			46.23 TB (71.27%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


2021-07-27 08:27:21,809 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: slave2:50010:DataXceiver error processing READ_BLOCK operation  src: /10.10.0.253:54500 dst: /10.10.0.54:50010
org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Replica not found for BP-820644543-10.10.0.254-1562028117422:blk_1076748201_3008249
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.getReplica(BlockSender.java:455

```

2.补录橡胶的主连数据

​	补录17年的数据， 先上传完		--ok

​	开始发送17年的数据



3.根目录满了？

减少目录：dfs.datanode.data.dir

退役一台然后修改data.dir,重新上线





## 20210728

```
1.检查下hadoop集群, hbase集群		--ok
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			63.54 TB
			12.51 TB (19.69%)
			46.01 TB (72.4%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


2021-07-27 08:27:21,809 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: slave2:50010:DataXceiver error processing READ_BLOCK operation  src: /10.10.0.253:54500 dst: /10.10.0.54:50010
org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Replica not found for BP-820644543-10.10.0.254-1562028117422:blk_1076748201_3008249
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.getReplica(BlockSender.java:455

get "hznc_mkdata:tickdata", "CF109_1627451657_1"
```

2.补录橡胶的主连数据

​	补录17年的数据， 先上传完		--ok

​	开始发送17年的数据



3.根目录满了？



```shell
先用keep-0测试下
1.退役keep-0
	vim excludes
	hadoop dfsadmin -refreshNodes
	# 等退役完成
	hadoop dfsadmin -report
	hadoop-daemon.st stop datanode
	hdfs dfsadmin -refreshNodes # 刷新一下
2.停掉hbase
	hbase-daemon.stop regionserver
	
3.修改datanode.dir,保留最大的就好
dfs.datanode.data.dir
# <value>file:///data/dfs,file:///ncdata/dfs,file:///ncdata1/dfs</value>
#  <value>file:///ncdata1/dfsdata</value>
# 备份原来的目录，先不要立即删除

4.启动datanode
启动regionserver 
观察

keep-0，slave2，master,keep-1,slave1
keep-0, slave2,master，keep-1	--ok

current: slave1


负载均衡

限制网络带宽：

hdfs dfsadmin -setBalancerBandwidth  5388608  # 5M 

start-balancer.sh -h # 查看帮助

start-balancer.sh -t 40 # datanode 之间使用率相差%40以上开始rebanlace
```



简单工厂模式

​	角色：

​			Factory(工厂)：负责创建对象

​			Product(抽象类产品)

​			ConcreteProduct(具体产品)

​	可以根据不同的参数选项由于工厂类创建对应的对象。如果有新的产品，实现新的产品然后配置到工厂类中。

装饰模式

​	角色：抽象组件角色（Component）和具体组件角色（Concrete Component）,装饰角色（Decorator）和具体装饰角色（Concrete Decorator）

​	通过创建装饰角色，接收组件角色作为成员变量，动态的扩展组件角色的功能。

java IO流的设计就用到了这个模式：FileInputStream&FileOutputStream，FileReader&FileWriter这些可以直接操作文件流是节点流，在装饰模式中是组件角色。

而BufferedInputStream, BufferedFileReader 这些是依赖节点流扩展缓冲功能，是具体装饰角色。具体实现就是BufferedInputStream 内部有成员变量InputStream in,初始化时候接收InutStream，在原来InputStream基础上增加缓冲功能。

​	apache tika 的parse实现也是使用装饰模式。

策略模式：

​	角色：

​		策略（Strategy），接口，定义了算法方法。

​		具体策略（Concrete Strategy）, 具体算法实现。

​		上下文（Context）, 依赖策略成员变量，委托策略成员变量调用算法。

定义一些算法并将算法封装起来，可以根据需要替换算法。

​	满足开闭原则的，如果需要新的算法只要增加新的算法实现并且实现。

​	应用：

线程池中就应用到了策略模式：

```java
public ThreadPoolExecutor(int corePoolSize,
                              int maximumPoolSize,
                              long keepAliveTime,
                              TimeUnit unit,
                              BlockingQueue<Runnable> workQueue,
                              ThreadFactory threadFactory,
                              RejectedExecutionHandler handler) // 拒绝策略{
    ThreadPoolExecutor实际就是上下文角色
```



## 20210729

```
1.检查下hadoop集群, hbase集群	
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			61.32 TB
			12.55 TB (20.47%)
			43.75 TB (71.36%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


2021-07-27 08:27:21,809 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: slave2:50010:DataXceiver error processing READ_BLOCK operation  src: /10.10.0.253:54500 dst: /10.10.0.54:50010
org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Replica not found for BP-820644543-10.10.0.254-1562028117422:blk_1076748201_3008249
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.getReplica(BlockSender.java:455

get "hznc_mkdata:tickdata", "CF109_1627451657_1"
```

2.补录橡胶的主连数据

​	补录17年的数据， 先上传完		--ok

​	开始发送17年的数据

```shell
hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.log.bad.lines=true -Dimporttsv.skip.bad.lines=false -Dimporttsv.separator=, -Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from' -Dimporttsv.bulk.output=/history/futures-out/2017/hznc_mkdata-tickdata/ \
hznc_mkdata:tickdata \
/history/futures/2017/hznc_mkdata-tickdata

```

3.根目录满了？		--ok



```shell
先用keep-0测试下
1.退役keep-0
	vim excludes
	hadoop dfsadmin -refreshNodes
	# 等退役完成
	hadoop dfsadmin -report
	hadoop-daemon.st stop datanode
	hdfs dfsadmin -refreshNodes # 刷新一下
2.停掉hbase
	hbase-daemon.stop regionserver
	
3.修改datanode.dir,保留最大的就好
dfs.datanode.data.dir
# <value>file:///data/dfs,file:///ncdata/dfs,file:///ncdata1/dfs</value>
#  <value>file:///ncdata1/dfsdata</value>
# 备份原来的目录，先不要立即删除

4.启动datanode
启动regionserver 
观察

keep-0，slave2，master,keep-1,slave1
keep-0, slave2,master，keep-1,slave1	--ok

TODO:

负载均衡

限制网络带宽：

hdfs dfsadmin -setBalancerBandwidth  5388608  # 5M 

start-balancer.sh -h # 查看帮助

start-balancer.sh -t 40 # datanode 之间使用率相差%40以上开始rebanlace
```

​	工厂方法

工厂方法模式的主要角色如下。

1. 抽象工厂（Abstract Factory）：提供了创建产品的接口，调用者通过它访问具体工厂的工厂方法 newProduct() 来创建产品。
2. 具体工厂（ConcreteFactory）：主要是实现抽象工厂中的抽象方法，完成具体产品的创建。
3. 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能。
4. 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间一一对应。

简单工厂模式违背了开闭原则，每次有新增加的产品时需要修改工厂内部的判断逻辑。工厂方法模式可以解决这个问题，只需要创建对应的工厂类即可。用户可以不关心具体的产品和实现只有知道是哪个工厂生产的。

但是缺点是每次新增产品都要新增对应的工厂类使系统设计复杂。

jdk中的工厂方法：

Collection中的iterator方法， 对于iterator来说Collection就是抽象工厂，定义了工厂方法iterator()，ArrayList,HashSet就是具体工厂，有各自的实现Iteractor()方法,

具体产品分别是ListIterator， LinkedKeyIterator。



​		抽象工厂模式

抽象工厂（AbstractFactory）模式的定义：是一种为访问类提供一个创建一组相关或相互依赖对象的接口，且访问类无须指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。

抽象工厂模式是工厂方法模式的升级版本，工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。

抽象工厂模式的主要角色如下。

1. 抽象工厂（Abstract Factory）：提供了创建产品的接口，它包含多个创建产品的方法 newProduct()，可以创建多个不同等级的产品。
2. 具体工厂（Concrete Factory）：主要是实现抽象工厂中的多个抽象方法，完成具体产品的创建。
3. 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能，抽象工厂模式有多个抽象产品。
4. 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间是多对一的关系。

```java
interface AbstractFactory { // 抽象工厂
    public Product1 newProduct1();
    public Product2 newProduct2();
}

class ConcreteFactory1 implements AbstractFactory { // 具体工厂
    public Product1 newProduct1() {
        System.out.println("具体工厂 1 生成-->具体产品 11...");
        return new ConcreteProduct11();
    }
    public Product2 newProduct2() {
        System.out.println("具体工厂 1 生成-->具体产品 21...");
        return new ConcreteProduct21();
    }
}
// 后面可以实现具体工厂2， 具体工厂3...
```

jdk中的抽象工厂模式：

jdbc接口java.sql.Connection类就是一个抽象工厂，定义了一些抽象产品，可以生产哪些产品：Statement，PreparedStatement

而mysql 的com.mysql.jdbc.ConnectionImpl就是具体工厂，生产com.mysql.jdbc.StatementImpl和com.mysql.jdbc.ServerPreparedStatement是具体产品。



5.k线bug修复		--ok



今天工作-20210729

1.17年商品数据继续补录，上传csv文件到hdfs

2.处理hbase服务器磁盘占满的问题		--ok

明天工作：

1.继续补录17年商品数据, 跑mapreduce任务生成hfile



## 20210730

```
1.检查下hadoop集群, hbase集群	
	echo "list" | hbase shell # 是否能列出来
	Hbase  UI页面地址：http://10.10.0.254:60010/master-status#compactStas  # 节点数至少5个， Dead Region Servers
	HDFS   UI页面地址：http://10.10.0.254:50070/dfshealth.html#tab-overview 
			61.32 TB
			13.42 TB (21.89%)
			44.73 TB (72.95%)
	检查进程：

            2481 HMaster
            18354 ResourceManager
            2180 DataNode
            1925 NameNode
            1845 JournalNode
            28327 Jps
            26696 Main
            30620 QuorumPeerMain
            28238 ThriftServer

	
2.检查程序运行和发送情况
10.10.10.104   administrator/Admin123 
	进程：datacenter-realtime-futures		晚上八点
    	  
	     
3.检查界面，行情软件
	如果有问题，启动：hbase-daemon.sh start thrift2

CLOSE_WAIT 47
ESTABLISHED 133
FIN_WAIT2 9
TIME_WAIT 33


2021-07-27 08:27:21,809 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: slave2:50010:DataXceiver error processing READ_BLOCK operation  src: /10.10.0.253:54500 dst: /10.10.0.54:50010
org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Replica not found for BP-820644543-10.10.0.254-1562028117422:blk_1076748201_3008249
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.getReplica(BlockSender.java:455

get "hznc_mkdata:tickdata", "CF109_1627451657_1"
```



2.补录17年商品数据

​	开始发送17年的数据

```shell
hbase  org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.log.bad.lines=true -Dimporttsv.skip.bad.lines=false -Dimporttsv.separator=, -Dimporttsv.columns='HBASE_ROW_KEY,cf_data:01cSymbol,cf_data:02dbClosePrice,cf_data:03dbHeightPrice,cf_data:04dbLowPrice,cf_data:05dbOpenPrice,cf_data:06dbSum,cf_data:07dbYTClosePrice,cf_data:08uTime,cf_data:09uVolume,cf_data:10uVolume_Sell,cf_data:11zpos,cf_data:12zpos_diff,cf_data:13avgPrice,cf_data:14tdHighestPrice,cf_data:15tdLowestPrice,cf_data:16UpperLimitPrice,cf_data:17LowerLimitPrice,cf_data:18BidPrice1,cf_data:19BidVolume1,cf_data:20AskPrice1,cf_data:21AskVolume1,cf_data:from' -Dimporttsv.bulk.output=/history/futures-out/2017/hznc_mkdata-tickdata/ \
hznc_mkdata:tickdata \
/history/futures/2017/hznc_mkdata-tickdata


hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /history/futures-out/2017/hznc_mkdata-15sdata  hznc_mkdata:15sdata

tick, 1s, 2s		--ok
current: 5s
nok: 3s

3s报错：

2021-07-30 15:27:06,651 INFO  [main] mapreduce.Job: Task Id : attempt_1627553602235_0004_m_000009_2, Status : FAILED
Error: java.lang.IllegalArgumentException: Can't read partitions file
        at org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner.setConf(TotalOrderPartitioner.java:116)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)
        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:702)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: Wrong number of partitions in keyset
        at org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner.setConf(TotalOrderPartitioner.java:90)
        ... 10 more
```

3.盘面资金一分钟一下周期保存		-- 下周测试一天

```
hznc_tgdata:amount
1_000300_10_1627259400


#ifndef nle_kdperiod#ifndef nle_kdperiodtypedef enum __nle_kdperiod { nlec_kdp_null = 0, nlec_kdp_01m, nlec_kdp_05m, nlec_kdp_15m, nlec_kdp_01d, nlec_kdp_01w, nlec_kdp_01mth, nlec_kdp_30m, nlec_kdp_60m, nlec_kdp_tick, nlec_kdp_01s, nlec_kdp_05s, nlec_kdp_10s, nlec_kdp_15s, nlec_kdp_30s, nlec_kdp_year, nlec_kdp_02s, nlec_kdp_03s, nlec_kdp_07s, nlec_kdp_end,}nle_kdperiod;#endif // !nle_kdperiod
```

4.负载均衡

限制网络带宽：

// 50M 

hdfs dfsadmin -setBalancerBandwidth  52428800

hdfs balancer -threshold 10 # datanode 之间使用率相差%10以上开始rebanlace





5.26和6.1股指主连

5.其他

​	算法		--ok

```java
class Solution {
    public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) {
        if (root==p||root==q) {
            return root;
        }
        if (root!=null){
            TreeNode lNode=lowestCommonAncestor(root.left,p,q);
            TreeNode rNode=lowestCommonAncestor(root.right,p,q);
            if (lNode!=null&&rNode!=null)
                return root;
            else if(lNode==null) {//两个都在右子树
                return rNode;
            }
            else { //两个都在左子树里面
                return lNode;
            }
        }
        return null;
    }
}
```

​	kafka

​	zero copy

​	磁盘读写原理，内存页，脏页

​	设计模式

​			代理模式

​			原型模式

todo：

《hbase原理和实践》

reactor模式：

​		https://www.cnblogs.com/crazymakercircle/p/9833847.html



搭建数仓，使用flink，kafka,redis做实时处理， hive离线分析。

.java bigdata

​		窃取模型

​		ConcurrentHashMap

​		List

计算集群的吞吐量，读写速度，最大容量，qps,网络，磁盘等

6.25.-6.28的主连数据同步

jstat, qps

查看连接:

netstat -anp | grep 8080 -c

查看进程的线程数：


    hbase优化：
    1.生产环境下关闭自动触发Major Compaction功能，改为手动在业务低峰期触发。
    一般生产环境下为了避免影响读写请求，会禁用自动触发major compaction。
    2.使用数据块编码


补k线程序有个小bug,kbug:11:29:59 , tick:13:00:01,那就会漏掉13:00:00的k线



学习搭建和使用hive

hbase 版本：1.3.1

hadoop版本：2.7.3





今天工作-20210728

1.17年商品数据补录		--ok

2.盘面资金数据1分钟以下周期存储

​		下周测试半天

下周工作：

1.16年商品数据补录

2.21年的股指主连补录

3.盘面资金1分钟以下数据存储

where '2020-05-26' >= start_date AND '2020-05-26' <= end_date and 



今日总结-20210730：

数据中心-1.1.2

​	1.17年商品数据补录 进度100%		--ok

   2 .盘面资金数据1分钟以下周期存储	60%
下周计划：

​	数据中心-1.1.2

  1.16年商品数据补录				0%

 	2.	21年的股指主连补录				0%		
 	3.	盘面资金1分钟以下数据存储     60%

项目风险：无



3.归并排序？

​	Blockingqueue



todo:

nagios

新的项目架构：

​	kafka + redis + flink + hbase

学习：

​	hive + spark + hadoop

源码：

​	kafka, hbase

SPI ,serviceLoader

hbase性能测试，吞吐量如何？

用flink去实现k线合并的程序

装一套hive,跟hbase集成

装一套监控的系统

看看：

开仓，平仓

盘面资金

大单数据

北向资金

策略

主力合约Tick数据

股指

期权

股票的开盘时间

主连

股票



代码问题：

​	日志问题，直接System.println.out

​	配置文件，ip端口直接写死的

​	打出来的jar包是用eclipse导出来的，应该用mvn install

​	依赖的jar包直接提交了

​	代码格式化问题，格式太乱，大量重复代码，没有用到的变量。

​	单元测试



## TODO



|      |                   |                   |
| ---- | ----------------- | ----------------- |
| 一   | 系统:java, 大数据 | now-20210801      |
| 二   | flink+数仓项目    | 20210801-20211001 |
| 三   | 收集和整理        | 20211001-20211015 |
| 四   | 实践              | 20211015          |



LinkedBlockingQueue 线程安全

算法和计算机原理

flink

clickhouse

mysql

数据仓库








​	